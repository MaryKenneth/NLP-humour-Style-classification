{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maryk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maryk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\maryk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from split_data_utils import train_test_spliting\n",
    "from data_preprocessing import lemmatize_text_with_pos, tokens\n",
    "from ngram_utils import generate_ngrams, build_ngram_vocab, bag_of_ngrams\n",
    "from tf_idf_embedding_utils import tf_idf, tokenise, build_vocabulary, log_count_terms, count_terms\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer data\n",
    "nltk.download('wordnet')  # Download the WordNet data\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (1010,)\n",
      "y_train int32\n",
      "x_test object\n",
      "y_test (253,)\n",
      "['4 ways to become a better risk taker'\n",
      " '“Never argue with stupid people, they will drag you down to their level and then beat you with experience.”'\n",
      " '“Worrying is like paying a debt you don’t owe.”' ...\n",
      " \"Worker dies at minnesota vikings' stadium construction site\"\n",
      " \"sharps' injuries could pose hiv, hepatitis risk to surgeons\"\n",
      " \"My set is full of them, but I have a go to bit about how awful it is being a fat chick with small tits that almost always saves me when I'm faltering.\"]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"Humour_style.xlsx\")   # Read Excel dataset \n",
    "df = df[['JOKES', 'LABELS']]              # Extract Only the Jokes and Labels Column\n",
    "df = df[:1263]  \n",
    "\n",
    "train_ratio = 0.8\n",
    "seed = 100\n",
    "x_train, x_test, y_train, y_test = train_test_spliting(df,train_ratio,seed)\n",
    "\n",
    "y_train = y_train.astype(\"int32\")\n",
    "y_test  = y_test.astype(\"int32\")\n",
    "print(\"x_train\",x_train.shape)   # Get the shape of the training features (Number of instance, number of features/column)\n",
    "print(\"y_train\",y_train.dtype)   # Get the shape of the training label (Number of instance, number of column)\n",
    "print(\"x_test\",x_test.dtype)\n",
    "print(\"y_test\",y_test.shape)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize each example in the train dataset\n",
    "lemmatized_x_train  = [lemmatize_text_with_pos(example) for example in x_train]\n",
    "\n",
    "# Lemmatize each example in the test dataset\n",
    "lemmatized_x_test  = [lemmatize_text_with_pos(example) for example in x_test]\n",
    "\n",
    "x_train = np.array(lemmatized_x_train )   # Convert Train data to Numpy Array \n",
    "x_test = np.array(lemmatized_x_test)      # Convert Test data to Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 11794\n",
      "Train Features Shape: (1010, 11794)\n",
      "Test Features Shape: (253, 11794)\n"
     ]
    }
   ],
   "source": [
    "### Get vocabulary size from train dataset\n",
    "train_vocab  = set()\n",
    "\n",
    "# Build the vocabulary from train N-grams\n",
    "for example in x_train:\n",
    "    vocabs = generate_ngrams(example,2)\n",
    "    train_vocab.update(vocabs)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(train_vocab)}\")\n",
    "\n",
    "# Convert train N-grams to vectors using the vocabulary\n",
    "def convert_to_vectors(data, vocab):\n",
    "    features = []\n",
    "    for example in data:\n",
    "        vectors = bag_of_ngrams(example, 2, vocab)\n",
    "        features.append(vectors)\n",
    "    return np.squeeze(np.array(features))\n",
    "\n",
    "# Convert train N-grams to vectors\n",
    "xtrain_features = convert_to_vectors(x_train, train_vocab)\n",
    "\n",
    "# Convert test N-grams to vectors using the same vocabulary\n",
    "xtest_features = convert_to_vectors(x_test, train_vocab)\n",
    "\n",
    "print(f\"Train Features Shape: {xtrain_features.shape}\")\n",
    "print(f\"Test Features Shape: {xtest_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preprocessing to Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([1010, 5])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Creates a Tensor from a numpy.ndarray.\n",
    "# Any changes you make to the tensor will reflect in the original NumPy array\n",
    "x_train_tensor = torch.from_numpy(xtrain_features).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "x_test_tensor = torch.from_numpy(xtest_features).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "## Changing labels to one-hot encoding to match output size\n",
    "y_train_tensor = y_train_tensor.long() # If y_train_tensor is of type float, convert it to integer first\n",
    "num_classes = len(torch.unique(y_train_tensor)) # Determine the number of classes\n",
    "y_train_tensor = F.one_hot(y_train_tensor, num_classes=num_classes) # Convert to one-hot encoding\n",
    "y_train_tensor = y_train_tensor.float() #Convert long type back to float as BCELOSS uses float\n",
    "\n",
    "# Ask PyTorch to store any computed gradients so that we can examine them\n",
    "x_train_tensor.requires_grad_(True)\n",
    "\n",
    "# should be \"None\" at the moment. It will only be filled later after you call backward()\n",
    "print(x_train_tensor.grad)\n",
    "\n",
    "# Use a GPU if it exists\n",
    "if torch.cuda.is_available():\n",
    "    x_train_tensor = x_train_tensor.to('cuda')\n",
    "    x_test_tensor = x_test_tensor.to('cuda')\n",
    "    y_train_tensor = y_train_tensor.to('cuda')\n",
    "\n",
    "print(y_train_tensor.shape)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworksPytorch(nn.Module):\n",
    "    def __init__(self, n_input, h1=200, h2 =250, h3=210, n_output=5):\n",
    "        super().__init__() #Instantiate our nn.Module\n",
    "        self.fully_connected1 = nn.Linear(n_input, h1)\n",
    "        self.fully_connected2 = nn.Linear(h1, h2)\n",
    "        self.fully_connected3 = nn.Linear(h2, h3)\n",
    "        self.out = nn.Linear(h3, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Performs Forward Pass\n",
    "        x = F.relu(self.fully_connected1(x))\n",
    "        x = F.tanh(self.fully_connected2(x))\n",
    "        x = F.relu(self.fully_connected3(x))\n",
    "        x = torch.softmax(self.out(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0198,  0.0347,  ..., -0.0162, -0.0335,  0.0443],\n",
      "        [-0.0687, -0.0646,  0.0107,  ...,  0.0671, -0.0221,  0.0557],\n",
      "        [ 0.0140,  0.0259,  0.0147,  ...,  0.0222,  0.0171,  0.0258],\n",
      "        [-0.0306,  0.0520,  0.0449,  ...,  0.0559, -0.0357,  0.0179],\n",
      "        [-0.0508, -0.0315, -0.0385,  ..., -0.0182,  0.0290,  0.0373]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0603,  0.0358,  0.0118,  0.0541, -0.0087], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "[Parameter containing:\n",
      "tensor([[-0.0015, -0.0006, -0.0068,  ...,  0.0010,  0.0180, -0.0109],\n",
      "        [-0.0087,  0.0131, -0.0139,  ..., -0.0162, -0.0045,  0.0034],\n",
      "        [-0.0133, -0.0166,  0.0082,  ..., -0.0174,  0.0134,  0.0085],\n",
      "        ...,\n",
      "        [ 0.0088,  0.0053, -0.0110,  ...,  0.0012, -0.0055, -0.0028],\n",
      "        [ 0.0013,  0.0065, -0.0037,  ...,  0.0161, -0.0048, -0.0143],\n",
      "        [-0.0048, -0.0018, -0.0053,  ...,  0.0055, -0.0117,  0.0155]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 9.2468e-03, -5.9733e-03,  2.5512e-04,  2.6465e-03, -4.4395e-03,\n",
      "         1.5911e-02, -1.4325e-02,  1.7352e-02, -1.1989e-02, -7.7240e-03,\n",
      "        -1.2904e-02, -1.5265e-02,  6.4578e-03,  1.7770e-02, -5.2170e-03,\n",
      "        -3.4949e-03,  2.0940e-03, -1.1205e-02,  1.2683e-02, -1.5260e-02,\n",
      "        -1.2391e-02, -5.7125e-03, -7.9739e-03, -1.2463e-02,  1.0369e-02,\n",
      "        -1.0574e-02, -2.6937e-03,  6.3094e-03,  6.8806e-03, -1.6623e-02,\n",
      "        -6.8270e-05,  6.0408e-03, -1.1886e-02, -9.3562e-03,  2.5431e-03,\n",
      "        -1.2063e-02,  6.1151e-03,  1.1316e-02, -2.7219e-03, -4.3081e-03,\n",
      "        -1.5934e-02,  4.6022e-03, -8.4765e-03,  1.2371e-02, -1.3736e-02,\n",
      "         7.3388e-03, -1.3931e-02, -5.2055e-03, -1.1448e-02,  7.0667e-04,\n",
      "         6.6513e-03, -2.0490e-03,  1.4366e-02, -5.3663e-03,  4.2255e-03,\n",
      "        -5.6358e-03, -6.4287e-03, -1.1437e-02,  3.3935e-03, -2.7823e-03,\n",
      "        -9.8435e-03, -1.7235e-02,  1.5267e-03, -3.7987e-03,  3.7279e-03,\n",
      "        -1.4138e-02,  2.9457e-03, -1.5268e-02,  1.7880e-02, -1.4445e-02,\n",
      "         8.3981e-03, -3.6838e-03, -1.6077e-02,  1.5410e-03, -1.0825e-02,\n",
      "         2.9898e-03, -1.4618e-02, -1.7328e-02, -1.6228e-02, -7.1851e-03,\n",
      "        -1.6496e-02,  5.7014e-03,  3.0604e-03,  1.1767e-02, -1.5068e-02,\n",
      "         4.6214e-03,  6.6428e-03, -2.0819e-03,  1.0549e-03, -1.5110e-02,\n",
      "        -1.6431e-02, -1.0779e-02,  1.3987e-02, -2.2942e-03,  1.1013e-02,\n",
      "         6.0567e-04, -1.6462e-04,  9.4016e-03,  1.5044e-02,  4.2792e-03,\n",
      "         7.7810e-03,  1.5220e-02,  1.6682e-02, -2.4073e-03, -1.6794e-02,\n",
      "        -8.0912e-03, -1.5327e-02,  1.2171e-02, -1.0636e-02,  1.2630e-02,\n",
      "         6.1848e-04, -1.7832e-02,  2.1170e-03,  1.5449e-02,  5.0401e-03,\n",
      "        -3.3774e-03, -2.9271e-03,  1.1366e-02, -3.7514e-03,  6.5594e-03,\n",
      "         5.7029e-03,  9.7820e-03, -1.7052e-02,  3.6987e-03,  5.5106e-03,\n",
      "        -9.4604e-03,  4.3286e-03, -7.9829e-03, -1.7219e-02,  5.6588e-03,\n",
      "        -1.0617e-02, -1.5164e-02, -4.8125e-03,  1.0711e-02, -6.4746e-03,\n",
      "         1.7876e-02, -5.6448e-03,  2.8452e-03,  1.4300e-02,  3.7076e-03,\n",
      "         6.3395e-03, -1.1645e-02,  1.7407e-02,  6.9105e-03,  1.0767e-02,\n",
      "        -3.5392e-03,  1.6575e-02, -9.1887e-03,  1.3030e-02,  9.5146e-05,\n",
      "        -3.2830e-03, -8.6062e-03, -7.4758e-03,  7.8142e-03, -4.9345e-04,\n",
      "         3.1707e-03, -3.1286e-03,  7.8703e-03, -8.5011e-03, -6.2883e-04,\n",
      "        -1.5322e-02, -9.2338e-03, -9.3921e-03,  8.8387e-03,  2.9244e-03,\n",
      "         1.0038e-02, -9.4442e-03,  1.7777e-02,  2.7897e-03,  1.1624e-03,\n",
      "         5.0217e-03,  2.9135e-03,  8.0467e-03,  5.3245e-03, -1.3039e-02,\n",
      "        -6.5180e-03, -9.6213e-03, -1.6270e-02,  2.9458e-03,  3.2256e-03,\n",
      "        -2.5026e-03, -1.4926e-02,  1.3780e-02,  9.6594e-03,  1.1696e-02,\n",
      "        -1.2134e-02,  2.0694e-03, -3.5125e-03, -3.1481e-03,  1.1193e-03,\n",
      "         1.5968e-02, -8.1425e-03, -1.0106e-02,  9.7199e-03, -1.7656e-02,\n",
      "        -1.0866e-02, -1.6347e-02, -1.6156e-03, -1.0973e-02,  7.1574e-03],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0639,  0.0640,  0.0379,  ..., -0.0143,  0.0447, -0.0143],\n",
      "        [ 0.0344, -0.0215,  0.0690,  ...,  0.0285, -0.0216, -0.0637],\n",
      "        [ 0.0638, -0.0545, -0.0227,  ...,  0.0684, -0.0416,  0.0398],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0110,  0.0621,  ...,  0.0253, -0.0569, -0.0446],\n",
      "        [ 0.0121, -0.0065, -0.0480,  ..., -0.0320, -0.0619, -0.0125],\n",
      "        [ 0.0245, -0.0646, -0.0243,  ..., -0.0252,  0.0518,  0.0297]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0436,  0.0535,  0.0071,  0.0172, -0.0516, -0.0199,  0.0546,  0.0059,\n",
      "         0.0406,  0.0577,  0.0409,  0.0263, -0.0484,  0.0031,  0.0574,  0.0319,\n",
      "        -0.0700, -0.0080, -0.0084, -0.0171, -0.0626,  0.0065,  0.0087, -0.0505,\n",
      "        -0.0390,  0.0010, -0.0160,  0.0279,  0.0490, -0.0303, -0.0303, -0.0355,\n",
      "         0.0494, -0.0536,  0.0558, -0.0130, -0.0426, -0.0448, -0.0368, -0.0232,\n",
      "        -0.0531, -0.0305,  0.0028,  0.0436,  0.0543,  0.0179,  0.0440,  0.0161,\n",
      "         0.0457, -0.0352,  0.0440,  0.0273,  0.0557,  0.0584,  0.0438,  0.0366,\n",
      "        -0.0662,  0.0629,  0.0268, -0.0147,  0.0067, -0.0054,  0.0446, -0.0240,\n",
      "        -0.0368, -0.0702,  0.0229, -0.0510, -0.0303, -0.0349,  0.0085, -0.0126,\n",
      "         0.0266,  0.0273,  0.0561, -0.0069,  0.0692,  0.0111,  0.0649, -0.0411,\n",
      "         0.0112,  0.0019,  0.0394,  0.0540,  0.0402, -0.0281,  0.0654, -0.0619,\n",
      "         0.0598, -0.0548,  0.0167,  0.0429, -0.0530,  0.0299, -0.0655,  0.0192,\n",
      "        -0.0049, -0.0452, -0.0696,  0.0030,  0.0503,  0.0359,  0.0641,  0.0476,\n",
      "        -0.0268, -0.0244, -0.0271, -0.0661,  0.0428, -0.0153,  0.0057,  0.0511,\n",
      "         0.0279, -0.0418,  0.0104,  0.0017, -0.0239, -0.0320,  0.0322,  0.0039,\n",
      "         0.0131, -0.0165,  0.0587, -0.0639, -0.0443, -0.0447,  0.0028, -0.0258,\n",
      "        -0.0212,  0.0108,  0.0052,  0.0588,  0.0008, -0.0536,  0.0209, -0.0668,\n",
      "         0.0593, -0.0616, -0.0606, -0.0279,  0.0611,  0.0518,  0.0580, -0.0272,\n",
      "         0.0046, -0.0513, -0.0593, -0.0660,  0.0597, -0.0119, -0.0577, -0.0472,\n",
      "         0.0366, -0.0666,  0.0157,  0.0225,  0.0051,  0.0226, -0.0015, -0.0564,\n",
      "        -0.0408,  0.0383, -0.0149,  0.0674, -0.0362, -0.0119,  0.0303,  0.0347,\n",
      "        -0.0348, -0.0605, -0.0257,  0.0576, -0.0462, -0.0256,  0.0686, -0.0397,\n",
      "        -0.0415, -0.0593, -0.0235,  0.0426, -0.0270, -0.0465,  0.0372, -0.0057,\n",
      "        -0.0426, -0.0019,  0.0658,  0.0054, -0.0524,  0.0381, -0.0624, -0.0551,\n",
      "         0.0216,  0.0044, -0.0345, -0.0007, -0.0534,  0.0100,  0.0376, -0.0583,\n",
      "         0.0675,  0.0093,  0.0452, -0.0067, -0.0646,  0.0551,  0.0196, -0.0667,\n",
      "         0.0523, -0.0551, -0.0192, -0.0153, -0.0400, -0.0449, -0.0448, -0.0674,\n",
      "        -0.0568, -0.0038, -0.0177,  0.0513, -0.0130,  0.0334,  0.0694, -0.0206,\n",
      "         0.0067,  0.0576,  0.0224, -0.0536, -0.0430, -0.0577, -0.0106,  0.0130,\n",
      "         0.0235,  0.0588, -0.0098, -0.0273,  0.0206,  0.0523, -0.0093, -0.0056,\n",
      "        -0.0005, -0.0693, -0.0155, -0.0426, -0.0510,  0.0273, -0.0660,  0.0580,\n",
      "        -0.0563, -0.0007], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0297,  0.0434, -0.0227,  ...,  0.0157,  0.0162, -0.0080],\n",
      "        [ 0.0270,  0.0318, -0.0510,  ..., -0.0549,  0.0462,  0.0044],\n",
      "        [ 0.0040,  0.0268, -0.0520,  ..., -0.0504, -0.0125, -0.0301],\n",
      "        ...,\n",
      "        [-0.0304, -0.0020,  0.0303,  ...,  0.0354, -0.0062,  0.0468],\n",
      "        [ 0.0359, -0.0135, -0.0508,  ...,  0.0514,  0.0148, -0.0312],\n",
      "        [-0.0602,  0.0498,  0.0032,  ...,  0.0175, -0.0355,  0.0127]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0212, -0.0500,  0.0193, -0.0309,  0.0505, -0.0126,  0.0516, -0.0392,\n",
      "        -0.0488,  0.0377,  0.0302,  0.0560, -0.0127,  0.0309, -0.0386,  0.0132,\n",
      "         0.0101,  0.0081,  0.0420,  0.0592,  0.0206, -0.0415, -0.0355, -0.0445,\n",
      "        -0.0088, -0.0447,  0.0302,  0.0042, -0.0205, -0.0349,  0.0382,  0.0627,\n",
      "        -0.0443, -0.0204, -0.0557, -0.0273,  0.0618,  0.0424,  0.0598, -0.0406,\n",
      "         0.0266,  0.0291, -0.0105,  0.0185,  0.0463, -0.0340,  0.0164,  0.0025,\n",
      "         0.0402, -0.0342, -0.0548, -0.0202,  0.0584,  0.0283, -0.0053,  0.0322,\n",
      "         0.0308,  0.0625, -0.0509,  0.0075, -0.0106, -0.0462,  0.0545,  0.0155,\n",
      "        -0.0081, -0.0295, -0.0520,  0.0127,  0.0138,  0.0053, -0.0632,  0.0529,\n",
      "         0.0618, -0.0021,  0.0410,  0.0355, -0.0206, -0.0002, -0.0515, -0.0496,\n",
      "        -0.0258,  0.0035, -0.0519,  0.0498,  0.0061, -0.0256, -0.0333,  0.0528,\n",
      "         0.0488, -0.0539, -0.0604, -0.0094, -0.0349, -0.0285,  0.0309, -0.0008,\n",
      "        -0.0578, -0.0142, -0.0211, -0.0120,  0.0460, -0.0508, -0.0303, -0.0285,\n",
      "         0.0198, -0.0481, -0.0113,  0.0273, -0.0425,  0.0382, -0.0250, -0.0559,\n",
      "         0.0622, -0.0521,  0.0068, -0.0625,  0.0074, -0.0485,  0.0244,  0.0495,\n",
      "        -0.0302, -0.0023,  0.0182,  0.0247, -0.0480,  0.0414, -0.0373, -0.0146,\n",
      "        -0.0625,  0.0429,  0.0582,  0.0424,  0.0389,  0.0361, -0.0012,  0.0573,\n",
      "        -0.0286,  0.0593,  0.0539,  0.0115,  0.0546, -0.0145, -0.0355,  0.0377,\n",
      "         0.0397, -0.0018, -0.0361, -0.0269,  0.0561, -0.0349,  0.0459, -0.0023,\n",
      "         0.0024,  0.0248, -0.0417,  0.0235, -0.0488, -0.0428, -0.0331,  0.0013,\n",
      "         0.0168,  0.0481, -0.0166,  0.0586,  0.0247,  0.0566,  0.0096, -0.0243,\n",
      "        -0.0287, -0.0349,  0.0416, -0.0121,  0.0518,  0.0444,  0.0135,  0.0309,\n",
      "         0.0550, -0.0064, -0.0245,  0.0613, -0.0148,  0.0274,  0.0143, -0.0358,\n",
      "        -0.0317,  0.0585, -0.0198, -0.0007,  0.0134, -0.0366,  0.0566, -0.0007,\n",
      "        -0.0578, -0.0294,  0.0563,  0.0221, -0.0489, -0.0576,  0.0451, -0.0462,\n",
      "         0.0178, -0.0224, -0.0112, -0.0084, -0.0055, -0.0157, -0.0181, -0.0057,\n",
      "        -0.0488, -0.0019], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0663, -0.0198,  0.0347,  ..., -0.0162, -0.0335,  0.0443],\n",
      "        [-0.0687, -0.0646,  0.0107,  ...,  0.0671, -0.0221,  0.0557],\n",
      "        [ 0.0140,  0.0259,  0.0147,  ...,  0.0222,  0.0171,  0.0258],\n",
      "        [-0.0306,  0.0520,  0.0449,  ...,  0.0559, -0.0357,  0.0179],\n",
      "        [-0.0508, -0.0315, -0.0385,  ..., -0.0182,  0.0290,  0.0373]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0603,  0.0358,  0.0118,  0.0541, -0.0087], device='cuda:0',\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#Pick a Manual seed for randomisation\n",
    "torch.manual_seed(10)\n",
    "\n",
    "input_n = xtrain_features.shape[1]\n",
    "py_model = NeuralNetworksPytorch(input_n)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    py_model.to('cuda')\n",
    "\n",
    "print(py_model.out.weight) # Print Weights\n",
    "print(py_model.out.bias)   # Print Bias\n",
    "print(list(py_model.parameters())) # Print LR parameters (Both weight and Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t L: 1.6091\n",
      "Epoch: 1\t L: 1.6089\n",
      "Epoch: 2\t L: 1.6086\n",
      "Epoch: 3\t L: 1.6084\n",
      "Epoch: 4\t L: 1.6082\n",
      "Epoch: 5\t L: 1.6080\n",
      "Epoch: 6\t L: 1.6078\n",
      "Epoch: 7\t L: 1.6076\n",
      "Epoch: 8\t L: 1.6073\n",
      "Epoch: 9\t L: 1.6071\n",
      "Epoch: 10\t L: 1.6069\n",
      "Epoch: 11\t L: 1.6066\n",
      "Epoch: 12\t L: 1.6063\n",
      "Epoch: 13\t L: 1.6061\n",
      "Epoch: 14\t L: 1.6058\n",
      "Epoch: 15\t L: 1.6055\n",
      "Epoch: 16\t L: 1.6052\n",
      "Epoch: 17\t L: 1.6049\n",
      "Epoch: 18\t L: 1.6045\n",
      "Epoch: 19\t L: 1.6042\n",
      "Epoch: 20\t L: 1.6038\n",
      "Epoch: 21\t L: 1.6034\n",
      "Epoch: 22\t L: 1.6030\n",
      "Epoch: 23\t L: 1.6025\n",
      "Epoch: 24\t L: 1.6020\n",
      "Epoch: 25\t L: 1.6015\n",
      "Epoch: 26\t L: 1.6010\n",
      "Epoch: 27\t L: 1.6005\n",
      "Epoch: 28\t L: 1.5999\n",
      "Epoch: 29\t L: 1.5992\n",
      "Epoch: 30\t L: 1.5986\n",
      "Epoch: 31\t L: 1.5979\n",
      "Epoch: 32\t L: 1.5972\n",
      "Epoch: 33\t L: 1.5964\n",
      "Epoch: 34\t L: 1.5956\n",
      "Epoch: 35\t L: 1.5947\n",
      "Epoch: 36\t L: 1.5938\n",
      "Epoch: 37\t L: 1.5928\n",
      "Epoch: 38\t L: 1.5917\n",
      "Epoch: 39\t L: 1.5906\n",
      "Epoch: 40\t L: 1.5895\n",
      "Epoch: 41\t L: 1.5883\n",
      "Epoch: 42\t L: 1.5870\n",
      "Epoch: 43\t L: 1.5856\n",
      "Epoch: 44\t L: 1.5841\n",
      "Epoch: 45\t L: 1.5826\n",
      "Epoch: 46\t L: 1.5809\n",
      "Epoch: 47\t L: 1.5792\n",
      "Epoch: 48\t L: 1.5773\n",
      "Epoch: 49\t L: 1.5754\n",
      "Epoch: 50\t L: 1.5733\n",
      "Epoch: 51\t L: 1.5711\n",
      "Epoch: 52\t L: 1.5688\n",
      "Epoch: 53\t L: 1.5664\n",
      "Epoch: 54\t L: 1.5638\n",
      "Epoch: 55\t L: 1.5611\n",
      "Epoch: 56\t L: 1.5582\n",
      "Epoch: 57\t L: 1.5552\n",
      "Epoch: 58\t L: 1.5521\n",
      "Epoch: 59\t L: 1.5488\n",
      "Epoch: 60\t L: 1.5453\n",
      "Epoch: 61\t L: 1.5417\n",
      "Epoch: 62\t L: 1.5379\n",
      "Epoch: 63\t L: 1.5339\n",
      "Epoch: 64\t L: 1.5298\n",
      "Epoch: 65\t L: 1.5254\n",
      "Epoch: 66\t L: 1.5209\n",
      "Epoch: 67\t L: 1.5162\n",
      "Epoch: 68\t L: 1.5112\n",
      "Epoch: 69\t L: 1.5061\n",
      "Epoch: 70\t L: 1.5008\n",
      "Epoch: 71\t L: 1.4952\n",
      "Epoch: 72\t L: 1.4894\n",
      "Epoch: 73\t L: 1.4834\n",
      "Epoch: 74\t L: 1.4772\n",
      "Epoch: 75\t L: 1.4707\n",
      "Epoch: 76\t L: 1.4641\n",
      "Epoch: 77\t L: 1.4571\n",
      "Epoch: 78\t L: 1.4500\n",
      "Epoch: 79\t L: 1.4426\n",
      "Epoch: 80\t L: 1.4350\n",
      "Epoch: 81\t L: 1.4272\n",
      "Epoch: 82\t L: 1.4191\n",
      "Epoch: 83\t L: 1.4109\n",
      "Epoch: 84\t L: 1.4025\n",
      "Epoch: 85\t L: 1.3939\n",
      "Epoch: 86\t L: 1.3851\n",
      "Epoch: 87\t L: 1.3762\n",
      "Epoch: 88\t L: 1.3672\n",
      "Epoch: 89\t L: 1.3581\n",
      "Epoch: 90\t L: 1.3490\n",
      "Epoch: 91\t L: 1.3398\n",
      "Epoch: 92\t L: 1.3305\n",
      "Epoch: 93\t L: 1.3213\n",
      "Epoch: 94\t L: 1.3122\n",
      "Epoch: 95\t L: 1.3031\n",
      "Epoch: 96\t L: 1.2941\n",
      "Epoch: 97\t L: 1.2852\n",
      "Epoch: 98\t L: 1.2764\n",
      "Epoch: 99\t L: 1.2678\n",
      "Epoch: 100\t L: 1.2594\n",
      "Epoch: 101\t L: 1.2512\n",
      "Epoch: 102\t L: 1.2432\n",
      "Epoch: 103\t L: 1.2354\n",
      "Epoch: 104\t L: 1.2278\n",
      "Epoch: 105\t L: 1.2205\n",
      "Epoch: 106\t L: 1.2135\n",
      "Epoch: 107\t L: 1.2067\n",
      "Epoch: 108\t L: 1.2001\n",
      "Epoch: 109\t L: 1.1938\n",
      "Epoch: 110\t L: 1.1877\n",
      "Epoch: 111\t L: 1.1819\n",
      "Epoch: 112\t L: 1.1764\n",
      "Epoch: 113\t L: 1.1710\n",
      "Epoch: 114\t L: 1.1659\n",
      "Epoch: 115\t L: 1.1610\n",
      "Epoch: 116\t L: 1.1564\n",
      "Epoch: 117\t L: 1.1519\n",
      "Epoch: 118\t L: 1.1477\n",
      "Epoch: 119\t L: 1.1436\n",
      "Epoch: 120\t L: 1.1398\n",
      "Epoch: 121\t L: 1.1361\n",
      "Epoch: 122\t L: 1.1326\n",
      "Epoch: 123\t L: 1.1292\n",
      "Epoch: 124\t L: 1.1260\n",
      "Epoch: 125\t L: 1.1230\n",
      "Epoch: 126\t L: 1.1200\n",
      "Epoch: 127\t L: 1.1172\n",
      "Epoch: 128\t L: 1.1146\n",
      "Epoch: 129\t L: 1.1120\n",
      "Epoch: 130\t L: 1.1096\n",
      "Epoch: 131\t L: 1.1072\n",
      "Epoch: 132\t L: 1.1050\n",
      "Epoch: 133\t L: 1.1028\n",
      "Epoch: 134\t L: 1.1008\n",
      "Epoch: 135\t L: 1.0988\n",
      "Epoch: 136\t L: 1.0969\n",
      "Epoch: 137\t L: 1.0950\n",
      "Epoch: 138\t L: 1.0932\n",
      "Epoch: 139\t L: 1.0915\n",
      "Epoch: 140\t L: 1.0898\n",
      "Epoch: 141\t L: 1.0882\n",
      "Epoch: 142\t L: 1.0866\n",
      "Epoch: 143\t L: 1.0850\n",
      "Epoch: 144\t L: 1.0835\n",
      "Epoch: 145\t L: 1.0820\n",
      "Epoch: 146\t L: 1.0805\n",
      "Epoch: 147\t L: 1.0790\n",
      "Epoch: 148\t L: 1.0776\n",
      "Epoch: 149\t L: 1.0761\n",
      "Epoch: 150\t L: 1.0747\n",
      "Epoch: 151\t L: 1.0732\n",
      "Epoch: 152\t L: 1.0717\n",
      "Epoch: 153\t L: 1.0702\n",
      "Epoch: 154\t L: 1.0687\n",
      "Epoch: 155\t L: 1.0671\n",
      "Epoch: 156\t L: 1.0655\n",
      "Epoch: 157\t L: 1.0638\n",
      "Epoch: 158\t L: 1.0621\n",
      "Epoch: 159\t L: 1.0603\n",
      "Epoch: 160\t L: 1.0584\n",
      "Epoch: 161\t L: 1.0565\n",
      "Epoch: 162\t L: 1.0544\n",
      "Epoch: 163\t L: 1.0522\n",
      "Epoch: 164\t L: 1.0499\n",
      "Epoch: 165\t L: 1.0475\n",
      "Epoch: 166\t L: 1.0449\n",
      "Epoch: 167\t L: 1.0423\n",
      "Epoch: 168\t L: 1.0395\n",
      "Epoch: 169\t L: 1.0366\n",
      "Epoch: 170\t L: 1.0336\n",
      "Epoch: 171\t L: 1.0305\n",
      "Epoch: 172\t L: 1.0274\n",
      "Epoch: 173\t L: 1.0242\n",
      "Epoch: 174\t L: 1.0211\n",
      "Epoch: 175\t L: 1.0179\n",
      "Epoch: 176\t L: 1.0149\n",
      "Epoch: 177\t L: 1.0119\n",
      "Epoch: 178\t L: 1.0091\n",
      "Epoch: 179\t L: 1.0065\n",
      "Epoch: 180\t L: 1.0039\n",
      "Epoch: 181\t L: 1.0014\n",
      "Epoch: 182\t L: 0.9989\n",
      "Epoch: 183\t L: 0.9964\n",
      "Epoch: 184\t L: 0.9938\n",
      "Epoch: 185\t L: 0.9913\n",
      "Epoch: 186\t L: 0.9887\n",
      "Epoch: 187\t L: 0.9861\n",
      "Epoch: 188\t L: 0.9836\n",
      "Epoch: 189\t L: 0.9811\n",
      "Epoch: 190\t L: 0.9787\n",
      "Epoch: 191\t L: 0.9763\n",
      "Epoch: 192\t L: 0.9740\n",
      "Epoch: 193\t L: 0.9718\n",
      "Epoch: 194\t L: 0.9697\n",
      "Epoch: 195\t L: 0.9677\n",
      "Epoch: 196\t L: 0.9657\n",
      "Epoch: 197\t L: 0.9638\n",
      "Epoch: 198\t L: 0.9620\n",
      "Epoch: 199\t L: 0.9602\n",
      "Epoch: 200\t L: 0.9584\n",
      "Epoch: 201\t L: 0.9568\n",
      "Epoch: 202\t L: 0.9551\n",
      "Epoch: 203\t L: 0.9536\n",
      "Epoch: 204\t L: 0.9520\n",
      "Epoch: 205\t L: 0.9506\n",
      "Epoch: 206\t L: 0.9492\n",
      "Epoch: 207\t L: 0.9479\n",
      "Epoch: 208\t L: 0.9466\n",
      "Epoch: 209\t L: 0.9453\n",
      "Epoch: 210\t L: 0.9441\n",
      "Epoch: 211\t L: 0.9430\n",
      "Epoch: 212\t L: 0.9419\n",
      "Epoch: 213\t L: 0.9409\n",
      "Epoch: 214\t L: 0.9398\n",
      "Epoch: 215\t L: 0.9389\n",
      "Epoch: 216\t L: 0.9379\n",
      "Epoch: 217\t L: 0.9370\n",
      "Epoch: 218\t L: 0.9361\n",
      "Epoch: 219\t L: 0.9353\n",
      "Epoch: 220\t L: 0.9345\n",
      "Epoch: 221\t L: 0.9337\n",
      "Epoch: 222\t L: 0.9330\n",
      "Epoch: 223\t L: 0.9323\n",
      "Epoch: 224\t L: 0.9316\n",
      "Epoch: 225\t L: 0.9309\n",
      "Epoch: 226\t L: 0.9303\n",
      "Epoch: 227\t L: 0.9296\n",
      "Epoch: 228\t L: 0.9290\n",
      "Epoch: 229\t L: 0.9285\n",
      "Epoch: 230\t L: 0.9279\n",
      "Epoch: 231\t L: 0.9274\n",
      "Epoch: 232\t L: 0.9269\n",
      "Epoch: 233\t L: 0.9264\n",
      "Epoch: 234\t L: 0.9259\n",
      "Epoch: 235\t L: 0.9254\n",
      "Epoch: 236\t L: 0.9250\n",
      "Epoch: 237\t L: 0.9246\n",
      "Epoch: 238\t L: 0.9241\n",
      "Epoch: 239\t L: 0.9237\n",
      "Epoch: 240\t L: 0.9234\n",
      "Epoch: 241\t L: 0.9230\n",
      "Epoch: 242\t L: 0.9226\n",
      "Epoch: 243\t L: 0.9223\n",
      "Epoch: 244\t L: 0.9219\n",
      "Epoch: 245\t L: 0.9216\n",
      "Epoch: 246\t L: 0.9213\n",
      "Epoch: 247\t L: 0.9209\n",
      "Epoch: 248\t L: 0.9206\n",
      "Epoch: 249\t L: 0.9203\n",
      "Epoch: 250\t L: 0.9200\n",
      "Epoch: 251\t L: 0.9198\n",
      "Epoch: 252\t L: 0.9195\n",
      "Epoch: 253\t L: 0.9192\n",
      "Epoch: 254\t L: 0.9189\n",
      "Epoch: 255\t L: 0.9187\n",
      "Epoch: 256\t L: 0.9184\n",
      "Epoch: 257\t L: 0.9182\n",
      "Epoch: 258\t L: 0.9180\n",
      "Epoch: 259\t L: 0.9177\n",
      "Epoch: 260\t L: 0.9175\n",
      "Epoch: 261\t L: 0.9173\n",
      "Epoch: 262\t L: 0.9171\n",
      "Epoch: 263\t L: 0.9169\n",
      "Epoch: 264\t L: 0.9167\n",
      "Epoch: 265\t L: 0.9165\n",
      "Epoch: 266\t L: 0.9163\n",
      "Epoch: 267\t L: 0.9161\n",
      "Epoch: 268\t L: 0.9160\n",
      "Epoch: 269\t L: 0.9158\n",
      "Epoch: 270\t L: 0.9156\n",
      "Epoch: 271\t L: 0.9155\n",
      "Epoch: 272\t L: 0.9153\n",
      "Epoch: 273\t L: 0.9152\n",
      "Epoch: 274\t L: 0.9151\n",
      "Epoch: 275\t L: 0.9149\n",
      "Epoch: 276\t L: 0.9148\n",
      "Epoch: 277\t L: 0.9147\n",
      "Epoch: 278\t L: 0.9146\n",
      "Epoch: 279\t L: 0.9144\n",
      "Epoch: 280\t L: 0.9143\n",
      "Epoch: 281\t L: 0.9142\n",
      "Epoch: 282\t L: 0.9141\n",
      "Epoch: 283\t L: 0.9140\n",
      "Epoch: 284\t L: 0.9139\n",
      "Epoch: 285\t L: 0.9138\n",
      "Epoch: 286\t L: 0.9137\n",
      "Epoch: 287\t L: 0.9136\n",
      "Epoch: 288\t L: 0.9135\n",
      "Epoch: 289\t L: 0.9134\n",
      "Epoch: 290\t L: 0.9134\n",
      "Epoch: 291\t L: 0.9133\n",
      "Epoch: 292\t L: 0.9132\n",
      "Epoch: 293\t L: 0.9131\n",
      "Epoch: 294\t L: 0.9130\n",
      "Epoch: 295\t L: 0.9130\n",
      "Epoch: 296\t L: 0.9129\n",
      "Epoch: 297\t L: 0.9128\n",
      "Epoch: 298\t L: 0.9127\n",
      "Epoch: 299\t L: 0.9127\n",
      "Epoch: 300\t L: 0.9126\n",
      "Epoch: 301\t L: 0.9125\n",
      "Epoch: 302\t L: 0.9125\n",
      "Epoch: 303\t L: 0.9124\n",
      "Epoch: 304\t L: 0.9123\n",
      "Epoch: 305\t L: 0.9123\n",
      "Epoch: 306\t L: 0.9122\n",
      "Epoch: 307\t L: 0.9122\n",
      "Epoch: 308\t L: 0.9121\n",
      "Epoch: 309\t L: 0.9120\n",
      "Epoch: 310\t L: 0.9120\n",
      "Epoch: 311\t L: 0.9119\n",
      "Epoch: 312\t L: 0.9119\n",
      "Epoch: 313\t L: 0.9118\n",
      "Epoch: 314\t L: 0.9118\n",
      "Epoch: 315\t L: 0.9117\n",
      "Epoch: 316\t L: 0.9117\n",
      "Epoch: 317\t L: 0.9116\n",
      "Epoch: 318\t L: 0.9116\n",
      "Epoch: 319\t L: 0.9115\n",
      "Epoch: 320\t L: 0.9115\n",
      "Epoch: 321\t L: 0.9114\n",
      "Epoch: 322\t L: 0.9114\n",
      "Epoch: 323\t L: 0.9113\n",
      "Epoch: 324\t L: 0.9113\n",
      "Epoch: 325\t L: 0.9112\n",
      "Epoch: 326\t L: 0.9112\n",
      "Epoch: 327\t L: 0.9112\n",
      "Epoch: 328\t L: 0.9111\n",
      "Epoch: 329\t L: 0.9111\n",
      "Epoch: 330\t L: 0.9110\n",
      "Epoch: 331\t L: 0.9110\n",
      "Epoch: 332\t L: 0.9109\n",
      "Epoch: 333\t L: 0.9109\n",
      "Epoch: 334\t L: 0.9109\n",
      "Epoch: 335\t L: 0.9108\n",
      "Epoch: 336\t L: 0.9108\n",
      "Epoch: 337\t L: 0.9107\n",
      "Epoch: 338\t L: 0.9107\n",
      "Epoch: 339\t L: 0.9106\n",
      "Epoch: 340\t L: 0.9106\n",
      "Epoch: 341\t L: 0.9106\n",
      "Epoch: 342\t L: 0.9105\n",
      "Epoch: 343\t L: 0.9105\n",
      "Epoch: 344\t L: 0.9104\n",
      "Epoch: 345\t L: 0.9104\n",
      "Epoch: 346\t L: 0.9103\n",
      "Epoch: 347\t L: 0.9103\n",
      "Epoch: 348\t L: 0.9103\n",
      "Epoch: 349\t L: 0.9102\n",
      "Epoch: 350\t L: 0.9102\n",
      "Epoch: 351\t L: 0.9101\n",
      "Epoch: 352\t L: 0.9101\n",
      "Epoch: 353\t L: 0.9100\n",
      "Epoch: 354\t L: 0.9100\n",
      "Epoch: 355\t L: 0.9099\n",
      "Epoch: 356\t L: 0.9099\n",
      "Epoch: 357\t L: 0.9098\n",
      "Epoch: 358\t L: 0.9098\n",
      "Epoch: 359\t L: 0.9097\n",
      "Epoch: 360\t L: 0.9097\n",
      "Epoch: 361\t L: 0.9096\n",
      "Epoch: 362\t L: 0.9096\n",
      "Epoch: 363\t L: 0.9096\n",
      "Epoch: 364\t L: 0.9095\n",
      "Epoch: 365\t L: 0.9095\n",
      "Epoch: 366\t L: 0.9095\n",
      "Epoch: 367\t L: 0.9094\n",
      "Epoch: 368\t L: 0.9094\n",
      "Epoch: 369\t L: 0.9094\n",
      "Epoch: 370\t L: 0.9093\n",
      "Epoch: 371\t L: 0.9093\n",
      "Epoch: 372\t L: 0.9093\n",
      "Epoch: 373\t L: 0.9092\n",
      "Epoch: 374\t L: 0.9092\n",
      "Epoch: 375\t L: 0.9092\n",
      "Epoch: 376\t L: 0.9092\n",
      "Epoch: 377\t L: 0.9091\n",
      "Epoch: 378\t L: 0.9091\n",
      "Epoch: 379\t L: 0.9091\n",
      "Epoch: 380\t L: 0.9091\n",
      "Epoch: 381\t L: 0.9090\n",
      "Epoch: 382\t L: 0.9090\n",
      "Epoch: 383\t L: 0.9090\n",
      "Epoch: 384\t L: 0.9090\n",
      "Epoch: 385\t L: 0.9090\n",
      "Epoch: 386\t L: 0.9089\n",
      "Epoch: 387\t L: 0.9089\n",
      "Epoch: 388\t L: 0.9089\n",
      "Epoch: 389\t L: 0.9089\n",
      "Epoch: 390\t L: 0.9089\n",
      "Epoch: 391\t L: 0.9088\n",
      "Epoch: 392\t L: 0.9088\n",
      "Epoch: 393\t L: 0.9088\n",
      "Epoch: 394\t L: 0.9088\n",
      "Epoch: 395\t L: 0.9088\n",
      "Epoch: 396\t L: 0.9088\n",
      "Epoch: 397\t L: 0.9087\n",
      "Epoch: 398\t L: 0.9087\n",
      "Epoch: 399\t L: 0.9087\n",
      "Epoch: 400\t L: 0.9087\n",
      "Epoch: 401\t L: 0.9087\n",
      "Epoch: 402\t L: 0.9087\n",
      "Epoch: 403\t L: 0.9086\n",
      "Epoch: 404\t L: 0.9086\n",
      "Epoch: 405\t L: 0.9086\n",
      "Epoch: 406\t L: 0.9086\n",
      "Epoch: 407\t L: 0.9086\n",
      "Epoch: 408\t L: 0.9086\n",
      "Epoch: 409\t L: 0.9086\n",
      "Epoch: 410\t L: 0.9085\n",
      "Epoch: 411\t L: 0.9085\n",
      "Epoch: 412\t L: 0.9085\n",
      "Epoch: 413\t L: 0.9085\n",
      "Epoch: 414\t L: 0.9085\n",
      "Epoch: 415\t L: 0.9085\n",
      "Epoch: 416\t L: 0.9085\n",
      "Epoch: 417\t L: 0.9085\n",
      "Epoch: 418\t L: 0.9084\n",
      "Epoch: 419\t L: 0.9084\n",
      "Epoch: 420\t L: 0.9084\n",
      "Epoch: 421\t L: 0.9084\n",
      "Epoch: 422\t L: 0.9084\n",
      "Epoch: 423\t L: 0.9084\n",
      "Epoch: 424\t L: 0.9084\n",
      "Epoch: 425\t L: 0.9084\n",
      "Epoch: 426\t L: 0.9083\n",
      "Epoch: 427\t L: 0.9083\n",
      "Epoch: 428\t L: 0.9083\n",
      "Epoch: 429\t L: 0.9083\n",
      "Epoch: 430\t L: 0.9083\n",
      "Epoch: 431\t L: 0.9083\n",
      "Epoch: 432\t L: 0.9083\n",
      "Epoch: 433\t L: 0.9083\n",
      "Epoch: 434\t L: 0.9083\n",
      "Epoch: 435\t L: 0.9082\n",
      "Epoch: 436\t L: 0.9082\n",
      "Epoch: 437\t L: 0.9082\n",
      "Epoch: 438\t L: 0.9082\n",
      "Epoch: 439\t L: 0.9082\n",
      "Epoch: 440\t L: 0.9082\n",
      "Epoch: 441\t L: 0.9082\n",
      "Epoch: 442\t L: 0.9082\n",
      "Epoch: 443\t L: 0.9082\n",
      "Epoch: 444\t L: 0.9082\n",
      "Epoch: 445\t L: 0.9082\n",
      "Epoch: 446\t L: 0.9081\n",
      "Epoch: 447\t L: 0.9081\n",
      "Epoch: 448\t L: 0.9081\n",
      "Epoch: 449\t L: 0.9081\n",
      "Epoch: 450\t L: 0.9081\n",
      "Epoch: 451\t L: 0.9081\n",
      "Epoch: 452\t L: 0.9081\n",
      "Epoch: 453\t L: 0.9081\n",
      "Epoch: 454\t L: 0.9081\n",
      "Epoch: 455\t L: 0.9081\n",
      "Epoch: 456\t L: 0.9081\n",
      "Epoch: 457\t L: 0.9080\n",
      "Epoch: 458\t L: 0.9080\n",
      "Epoch: 459\t L: 0.9080\n",
      "Epoch: 460\t L: 0.9080\n",
      "Epoch: 461\t L: 0.9080\n",
      "Epoch: 462\t L: 0.9080\n",
      "Epoch: 463\t L: 0.9080\n",
      "Epoch: 464\t L: 0.9080\n",
      "Epoch: 465\t L: 0.9080\n",
      "Epoch: 466\t L: 0.9080\n",
      "Epoch: 467\t L: 0.9080\n",
      "Epoch: 468\t L: 0.9080\n",
      "Epoch: 469\t L: 0.9080\n",
      "Epoch: 470\t L: 0.9079\n",
      "Epoch: 471\t L: 0.9079\n",
      "Epoch: 472\t L: 0.9079\n",
      "Epoch: 473\t L: 0.9079\n",
      "Epoch: 474\t L: 0.9079\n",
      "Epoch: 475\t L: 0.9079\n",
      "Epoch: 476\t L: 0.9079\n",
      "Epoch: 477\t L: 0.9079\n",
      "Epoch: 478\t L: 0.9079\n",
      "Epoch: 479\t L: 0.9079\n",
      "Epoch: 480\t L: 0.9079\n",
      "Epoch: 481\t L: 0.9079\n",
      "Epoch: 482\t L: 0.9079\n",
      "Epoch: 483\t L: 0.9079\n",
      "Epoch: 484\t L: 0.9079\n",
      "Epoch: 485\t L: 0.9078\n",
      "Epoch: 486\t L: 0.9078\n",
      "Epoch: 487\t L: 0.9078\n",
      "Epoch: 488\t L: 0.9078\n",
      "Epoch: 489\t L: 0.9078\n",
      "Epoch: 490\t L: 0.9078\n",
      "Epoch: 491\t L: 0.9078\n",
      "Epoch: 492\t L: 0.9078\n",
      "Epoch: 493\t L: 0.9078\n",
      "Epoch: 494\t L: 0.9078\n",
      "Epoch: 495\t L: 0.9078\n",
      "Epoch: 496\t L: 0.9078\n",
      "Epoch: 497\t L: 0.9078\n",
      "Epoch: 498\t L: 0.9078\n",
      "Epoch: 499\t L: 0.9078\n",
      "Epoch: 500\t L: 0.9078\n",
      "Epoch: 501\t L: 0.9078\n",
      "Epoch: 502\t L: 0.9077\n",
      "Epoch: 503\t L: 0.9077\n",
      "Epoch: 504\t L: 0.9077\n",
      "Epoch: 505\t L: 0.9077\n",
      "Epoch: 506\t L: 0.9077\n",
      "Epoch: 507\t L: 0.9077\n",
      "Epoch: 508\t L: 0.9077\n",
      "Epoch: 509\t L: 0.9077\n",
      "Epoch: 510\t L: 0.9077\n",
      "Epoch: 511\t L: 0.9077\n",
      "Epoch: 512\t L: 0.9077\n",
      "Epoch: 513\t L: 0.9077\n",
      "Epoch: 514\t L: 0.9077\n",
      "Epoch: 515\t L: 0.9077\n",
      "Epoch: 516\t L: 0.9077\n",
      "Epoch: 517\t L: 0.9077\n",
      "Epoch: 518\t L: 0.9077\n",
      "Epoch: 519\t L: 0.9077\n",
      "Epoch: 520\t L: 0.9077\n",
      "Epoch: 521\t L: 0.9076\n",
      "Epoch: 522\t L: 0.9076\n",
      "Epoch: 523\t L: 0.9076\n",
      "Epoch: 524\t L: 0.9076\n",
      "Epoch: 525\t L: 0.9076\n",
      "Epoch: 526\t L: 0.9076\n",
      "Epoch: 527\t L: 0.9076\n",
      "Epoch: 528\t L: 0.9076\n",
      "Epoch: 529\t L: 0.9076\n",
      "Epoch: 530\t L: 0.9076\n",
      "Epoch: 531\t L: 0.9076\n",
      "Epoch: 532\t L: 0.9076\n",
      "Epoch: 533\t L: 0.9076\n",
      "Epoch: 534\t L: 0.9076\n",
      "Epoch: 535\t L: 0.9076\n",
      "Epoch: 536\t L: 0.9076\n",
      "Epoch: 537\t L: 0.9076\n",
      "Epoch: 538\t L: 0.9076\n",
      "Epoch: 539\t L: 0.9076\n",
      "Epoch: 540\t L: 0.9076\n",
      "Epoch: 541\t L: 0.9076\n",
      "Epoch: 542\t L: 0.9076\n",
      "Epoch: 543\t L: 0.9076\n",
      "Epoch: 544\t L: 0.9076\n",
      "Epoch: 545\t L: 0.9075\n",
      "Epoch: 546\t L: 0.9075\n",
      "Epoch: 547\t L: 0.9075\n",
      "Epoch: 548\t L: 0.9075\n",
      "Epoch: 549\t L: 0.9075\n",
      "Epoch: 550\t L: 0.9075\n",
      "Epoch: 551\t L: 0.9075\n",
      "Epoch: 552\t L: 0.9075\n",
      "Epoch: 553\t L: 0.9075\n",
      "Epoch: 554\t L: 0.9075\n",
      "Epoch: 555\t L: 0.9075\n",
      "Epoch: 556\t L: 0.9075\n",
      "Epoch: 557\t L: 0.9075\n",
      "Epoch: 558\t L: 0.9075\n",
      "Epoch: 559\t L: 0.9075\n",
      "Epoch: 560\t L: 0.9075\n",
      "Epoch: 561\t L: 0.9075\n",
      "Epoch: 562\t L: 0.9075\n",
      "Epoch: 563\t L: 0.9075\n",
      "Epoch: 564\t L: 0.9075\n",
      "Epoch: 565\t L: 0.9075\n",
      "Epoch: 566\t L: 0.9075\n",
      "Epoch: 567\t L: 0.9075\n",
      "Epoch: 568\t L: 0.9075\n",
      "Epoch: 569\t L: 0.9075\n",
      "Epoch: 570\t L: 0.9075\n",
      "Epoch: 571\t L: 0.9075\n",
      "Epoch: 572\t L: 0.9075\n",
      "Epoch: 573\t L: 0.9075\n",
      "Epoch: 574\t L: 0.9075\n",
      "Epoch: 575\t L: 0.9074\n",
      "Epoch: 576\t L: 0.9074\n",
      "Epoch: 577\t L: 0.9074\n",
      "Epoch: 578\t L: 0.9074\n",
      "Epoch: 579\t L: 0.9074\n",
      "Epoch: 580\t L: 0.9074\n",
      "Epoch: 581\t L: 0.9074\n",
      "Epoch: 582\t L: 0.9074\n",
      "Epoch: 583\t L: 0.9074\n",
      "Epoch: 584\t L: 0.9074\n",
      "Epoch: 585\t L: 0.9074\n",
      "Epoch: 586\t L: 0.9074\n",
      "Epoch: 587\t L: 0.9074\n",
      "Epoch: 588\t L: 0.9074\n",
      "Epoch: 589\t L: 0.9074\n",
      "Epoch: 590\t L: 0.9074\n",
      "Epoch: 591\t L: 0.9074\n",
      "Epoch: 592\t L: 0.9074\n",
      "Epoch: 593\t L: 0.9074\n",
      "Epoch: 594\t L: 0.9074\n",
      "Epoch: 595\t L: 0.9074\n",
      "Epoch: 596\t L: 0.9074\n",
      "Epoch: 597\t L: 0.9074\n",
      "Epoch: 598\t L: 0.9074\n",
      "Epoch: 599\t L: 0.9074\n",
      "Epoch: 600\t L: 0.9074\n",
      "Epoch: 601\t L: 0.9074\n",
      "Epoch: 602\t L: 0.9074\n",
      "Epoch: 603\t L: 0.9074\n",
      "Epoch: 604\t L: 0.9074\n",
      "Epoch: 605\t L: 0.9074\n",
      "Epoch: 606\t L: 0.9074\n",
      "Epoch: 607\t L: 0.9074\n",
      "Epoch: 608\t L: 0.9074\n",
      "Epoch: 609\t L: 0.9074\n",
      "Epoch: 610\t L: 0.9074\n",
      "Epoch: 611\t L: 0.9074\n",
      "Epoch: 612\t L: 0.9073\n",
      "Epoch: 613\t L: 0.9073\n",
      "Epoch: 614\t L: 0.9073\n",
      "Epoch: 615\t L: 0.9073\n",
      "Epoch: 616\t L: 0.9073\n",
      "Epoch: 617\t L: 0.9073\n",
      "Epoch: 618\t L: 0.9073\n",
      "Epoch: 619\t L: 0.9073\n",
      "Epoch: 620\t L: 0.9073\n",
      "Epoch: 621\t L: 0.9073\n",
      "Epoch: 622\t L: 0.9073\n",
      "Epoch: 623\t L: 0.9073\n",
      "Epoch: 624\t L: 0.9073\n",
      "Epoch: 625\t L: 0.9073\n",
      "Epoch: 626\t L: 0.9073\n",
      "Epoch: 627\t L: 0.9073\n",
      "Epoch: 628\t L: 0.9073\n",
      "Epoch: 629\t L: 0.9073\n",
      "Epoch: 630\t L: 0.9073\n",
      "Epoch: 631\t L: 0.9073\n",
      "Epoch: 632\t L: 0.9073\n",
      "Epoch: 633\t L: 0.9073\n",
      "Epoch: 634\t L: 0.9073\n",
      "Epoch: 635\t L: 0.9073\n",
      "Epoch: 636\t L: 0.9073\n",
      "Epoch: 637\t L: 0.9073\n",
      "Epoch: 638\t L: 0.9073\n",
      "Epoch: 639\t L: 0.9073\n",
      "Epoch: 640\t L: 0.9073\n",
      "Epoch: 641\t L: 0.9073\n",
      "Epoch: 642\t L: 0.9073\n",
      "Epoch: 643\t L: 0.9073\n",
      "Epoch: 644\t L: 0.9073\n",
      "Epoch: 645\t L: 0.9073\n",
      "Epoch: 646\t L: 0.9073\n",
      "Epoch: 647\t L: 0.9073\n",
      "Epoch: 648\t L: 0.9073\n",
      "Epoch: 649\t L: 0.9073\n",
      "Epoch: 650\t L: 0.9073\n",
      "Epoch: 651\t L: 0.9073\n",
      "Epoch: 652\t L: 0.9073\n",
      "Epoch: 653\t L: 0.9073\n",
      "Epoch: 654\t L: 0.9073\n",
      "Epoch: 655\t L: 0.9073\n",
      "Epoch: 656\t L: 0.9073\n",
      "Epoch: 657\t L: 0.9073\n",
      "Epoch: 658\t L: 0.9073\n",
      "Epoch: 659\t L: 0.9073\n",
      "Epoch: 660\t L: 0.9073\n",
      "Epoch: 661\t L: 0.9073\n",
      "Epoch: 662\t L: 0.9072\n",
      "Epoch: 663\t L: 0.9072\n",
      "Epoch: 664\t L: 0.9072\n",
      "Epoch: 665\t L: 0.9072\n",
      "Epoch: 666\t L: 0.9072\n",
      "Epoch: 667\t L: 0.9072\n",
      "Epoch: 668\t L: 0.9072\n",
      "Epoch: 669\t L: 0.9072\n",
      "Epoch: 670\t L: 0.9072\n",
      "Epoch: 671\t L: 0.9072\n",
      "Epoch: 672\t L: 0.9072\n",
      "Epoch: 673\t L: 0.9072\n",
      "Epoch: 674\t L: 0.9072\n",
      "Epoch: 675\t L: 0.9072\n",
      "Epoch: 676\t L: 0.9072\n",
      "Epoch: 677\t L: 0.9072\n",
      "Epoch: 678\t L: 0.9072\n",
      "Epoch: 679\t L: 0.9072\n",
      "Epoch: 680\t L: 0.9072\n",
      "Epoch: 681\t L: 0.9072\n",
      "Epoch: 682\t L: 0.9072\n",
      "Epoch: 683\t L: 0.9072\n",
      "Epoch: 684\t L: 0.9072\n",
      "Epoch: 685\t L: 0.9072\n",
      "Epoch: 686\t L: 0.9072\n",
      "Epoch: 687\t L: 0.9072\n",
      "Epoch: 688\t L: 0.9072\n",
      "Epoch: 689\t L: 0.9072\n",
      "Epoch: 690\t L: 0.9072\n",
      "Epoch: 691\t L: 0.9072\n",
      "Epoch: 692\t L: 0.9072\n",
      "Epoch: 693\t L: 0.9072\n",
      "Epoch: 694\t L: 0.9072\n",
      "Epoch: 695\t L: 0.9072\n",
      "Epoch: 696\t L: 0.9072\n",
      "Epoch: 697\t L: 0.9072\n",
      "Epoch: 698\t L: 0.9072\n",
      "Epoch: 699\t L: 0.9072\n",
      "Epoch: 700\t L: 0.9072\n",
      "Epoch: 701\t L: 0.9072\n",
      "Epoch: 702\t L: 0.9072\n",
      "Epoch: 703\t L: 0.9072\n",
      "Epoch: 704\t L: 0.9072\n",
      "Epoch: 705\t L: 0.9072\n",
      "Epoch: 706\t L: 0.9072\n",
      "Epoch: 707\t L: 0.9072\n",
      "Epoch: 708\t L: 0.9072\n",
      "Epoch: 709\t L: 0.9072\n",
      "Epoch: 710\t L: 0.9072\n",
      "Epoch: 711\t L: 0.9072\n",
      "Epoch: 712\t L: 0.9072\n",
      "Epoch: 713\t L: 0.9072\n",
      "Epoch: 714\t L: 0.9072\n",
      "Epoch: 715\t L: 0.9072\n",
      "Epoch: 716\t L: 0.9072\n",
      "Epoch: 717\t L: 0.9072\n",
      "Epoch: 718\t L: 0.9072\n",
      "Epoch: 719\t L: 0.9072\n",
      "Epoch: 720\t L: 0.9072\n",
      "Epoch: 721\t L: 0.9072\n",
      "Epoch: 722\t L: 0.9072\n",
      "Epoch: 723\t L: 0.9072\n",
      "Epoch: 724\t L: 0.9072\n",
      "Epoch: 725\t L: 0.9072\n",
      "Epoch: 726\t L: 0.9072\n",
      "Epoch: 727\t L: 0.9072\n",
      "Epoch: 728\t L: 0.9072\n",
      "Epoch: 729\t L: 0.9072\n",
      "Epoch: 730\t L: 0.9072\n",
      "Epoch: 731\t L: 0.9072\n",
      "Epoch: 732\t L: 0.9071\n",
      "Epoch: 733\t L: 0.9071\n",
      "Epoch: 734\t L: 0.9071\n",
      "Epoch: 735\t L: 0.9071\n",
      "Epoch: 736\t L: 0.9071\n",
      "Epoch: 737\t L: 0.9071\n",
      "Epoch: 738\t L: 0.9071\n",
      "Epoch: 739\t L: 0.9071\n",
      "Epoch: 740\t L: 0.9071\n",
      "Epoch: 741\t L: 0.9071\n",
      "Epoch: 742\t L: 0.9071\n",
      "Epoch: 743\t L: 0.9071\n",
      "Epoch: 744\t L: 0.9071\n",
      "Epoch: 745\t L: 0.9071\n",
      "Epoch: 746\t L: 0.9071\n",
      "Epoch: 747\t L: 0.9071\n",
      "Epoch: 748\t L: 0.9071\n",
      "Epoch: 749\t L: 0.9071\n",
      "Epoch: 750\t L: 0.9071\n",
      "Epoch: 751\t L: 0.9071\n",
      "Epoch: 752\t L: 0.9071\n",
      "Epoch: 753\t L: 0.9071\n",
      "Epoch: 754\t L: 0.9071\n",
      "Epoch: 755\t L: 0.9071\n",
      "Epoch: 756\t L: 0.9071\n",
      "Epoch: 757\t L: 0.9071\n",
      "Epoch: 758\t L: 0.9071\n",
      "Epoch: 759\t L: 0.9071\n",
      "Epoch: 760\t L: 0.9071\n",
      "Epoch: 761\t L: 0.9071\n",
      "Epoch: 762\t L: 0.9071\n",
      "Epoch: 763\t L: 0.9071\n",
      "Epoch: 764\t L: 0.9071\n",
      "Epoch: 765\t L: 0.9071\n",
      "Epoch: 766\t L: 0.9071\n",
      "Epoch: 767\t L: 0.9071\n",
      "Epoch: 768\t L: 0.9071\n",
      "Epoch: 769\t L: 0.9071\n",
      "Epoch: 770\t L: 0.9071\n",
      "Epoch: 771\t L: 0.9071\n",
      "Epoch: 772\t L: 0.9071\n",
      "Epoch: 773\t L: 0.9071\n",
      "Epoch: 774\t L: 0.9071\n",
      "Epoch: 775\t L: 0.9071\n",
      "Epoch: 776\t L: 0.9071\n",
      "Epoch: 777\t L: 0.9071\n",
      "Epoch: 778\t L: 0.9071\n",
      "Epoch: 779\t L: 0.9071\n",
      "Epoch: 780\t L: 0.9071\n",
      "Epoch: 781\t L: 0.9071\n",
      "Epoch: 782\t L: 0.9071\n",
      "Epoch: 783\t L: 0.9071\n",
      "Epoch: 784\t L: 0.9071\n",
      "Epoch: 785\t L: 0.9071\n",
      "Epoch: 786\t L: 0.9071\n",
      "Epoch: 787\t L: 0.9071\n",
      "Epoch: 788\t L: 0.9071\n",
      "Epoch: 789\t L: 0.9071\n",
      "Epoch: 790\t L: 0.9071\n",
      "Epoch: 791\t L: 0.9071\n",
      "Epoch: 792\t L: 0.9071\n",
      "Epoch: 793\t L: 0.9071\n",
      "Epoch: 794\t L: 0.9071\n",
      "Epoch: 795\t L: 0.9071\n",
      "Epoch: 796\t L: 0.9071\n",
      "Epoch: 797\t L: 0.9071\n",
      "Epoch: 798\t L: 0.9071\n",
      "Epoch: 799\t L: 0.9071\n",
      "Epoch: 800\t L: 0.9071\n",
      "Epoch: 801\t L: 0.9071\n",
      "Epoch: 802\t L: 0.9071\n",
      "Epoch: 803\t L: 0.9071\n",
      "Epoch: 804\t L: 0.9071\n",
      "Epoch: 805\t L: 0.9071\n",
      "Epoch: 806\t L: 0.9071\n",
      "Epoch: 807\t L: 0.9071\n",
      "Epoch: 808\t L: 0.9071\n",
      "Epoch: 809\t L: 0.9071\n",
      "Epoch: 810\t L: 0.9071\n",
      "Epoch: 811\t L: 0.9071\n",
      "Epoch: 812\t L: 0.9071\n",
      "Epoch: 813\t L: 0.9071\n",
      "Epoch: 814\t L: 0.9071\n",
      "Epoch: 815\t L: 0.9071\n",
      "Epoch: 816\t L: 0.9071\n",
      "Epoch: 817\t L: 0.9071\n",
      "Epoch: 818\t L: 0.9071\n",
      "Epoch: 819\t L: 0.9071\n",
      "Epoch: 820\t L: 0.9071\n",
      "Epoch: 821\t L: 0.9071\n",
      "Epoch: 822\t L: 0.9071\n",
      "Epoch: 823\t L: 0.9071\n",
      "Epoch: 824\t L: 0.9071\n",
      "Epoch: 825\t L: 0.9071\n",
      "Epoch: 826\t L: 0.9071\n",
      "Epoch: 827\t L: 0.9071\n",
      "Epoch: 828\t L: 0.9071\n",
      "Epoch: 829\t L: 0.9071\n",
      "Epoch: 830\t L: 0.9071\n",
      "Epoch: 831\t L: 0.9071\n",
      "Epoch: 832\t L: 0.9071\n",
      "Epoch: 833\t L: 0.9071\n",
      "Epoch: 834\t L: 0.9071\n",
      "Epoch: 835\t L: 0.9071\n",
      "Epoch: 836\t L: 0.9071\n",
      "Epoch: 837\t L: 0.9071\n",
      "Epoch: 838\t L: 0.9071\n",
      "Epoch: 839\t L: 0.9071\n",
      "Epoch: 840\t L: 0.9070\n",
      "Epoch: 841\t L: 0.9070\n",
      "Epoch: 842\t L: 0.9070\n",
      "Epoch: 843\t L: 0.9070\n",
      "Epoch: 844\t L: 0.9070\n",
      "Epoch: 845\t L: 0.9070\n",
      "Epoch: 846\t L: 0.9070\n",
      "Epoch: 847\t L: 0.9070\n",
      "Epoch: 848\t L: 0.9070\n",
      "Epoch: 849\t L: 0.9070\n",
      "Epoch: 850\t L: 0.9070\n",
      "Epoch: 851\t L: 0.9070\n",
      "Epoch: 852\t L: 0.9070\n",
      "Epoch: 853\t L: 0.9070\n",
      "Epoch: 854\t L: 0.9070\n",
      "Epoch: 855\t L: 0.9070\n",
      "Epoch: 856\t L: 0.9070\n",
      "Epoch: 857\t L: 0.9070\n",
      "Epoch: 858\t L: 0.9070\n",
      "Epoch: 859\t L: 0.9070\n",
      "Epoch: 860\t L: 0.9070\n",
      "Epoch: 861\t L: 0.9070\n",
      "Epoch: 862\t L: 0.9070\n",
      "Epoch: 863\t L: 0.9070\n",
      "Epoch: 864\t L: 0.9070\n",
      "Epoch: 865\t L: 0.9070\n",
      "Epoch: 866\t L: 0.9070\n",
      "Epoch: 867\t L: 0.9070\n",
      "Epoch: 868\t L: 0.9070\n",
      "Epoch: 869\t L: 0.9070\n",
      "Epoch: 870\t L: 0.9070\n",
      "Epoch: 871\t L: 0.9070\n",
      "Epoch: 872\t L: 0.9070\n",
      "Epoch: 873\t L: 0.9070\n",
      "Epoch: 874\t L: 0.9070\n",
      "Epoch: 875\t L: 0.9070\n",
      "Epoch: 876\t L: 0.9070\n",
      "Epoch: 877\t L: 0.9070\n",
      "Epoch: 878\t L: 0.9070\n",
      "Epoch: 879\t L: 0.9070\n",
      "Epoch: 880\t L: 0.9070\n",
      "Epoch: 881\t L: 0.9070\n",
      "Epoch: 882\t L: 0.9070\n",
      "Epoch: 883\t L: 0.9070\n",
      "Epoch: 884\t L: 0.9070\n",
      "Epoch: 885\t L: 0.9070\n",
      "Epoch: 886\t L: 0.9070\n",
      "Epoch: 887\t L: 0.9070\n",
      "Epoch: 888\t L: 0.9070\n",
      "Epoch: 889\t L: 0.9070\n",
      "Epoch: 890\t L: 0.9070\n",
      "Epoch: 891\t L: 0.9070\n",
      "Epoch: 892\t L: 0.9070\n",
      "Epoch: 893\t L: 0.9070\n",
      "Epoch: 894\t L: 0.9070\n",
      "Epoch: 895\t L: 0.9070\n",
      "Epoch: 896\t L: 0.9070\n",
      "Epoch: 897\t L: 0.9070\n",
      "Epoch: 898\t L: 0.9070\n",
      "Epoch: 899\t L: 0.9070\n",
      "Epoch: 900\t L: 0.9070\n",
      "Epoch: 901\t L: 0.9070\n",
      "Epoch: 902\t L: 0.9070\n",
      "Epoch: 903\t L: 0.9070\n",
      "Epoch: 904\t L: 0.9070\n",
      "Epoch: 905\t L: 0.9070\n",
      "Epoch: 906\t L: 0.9070\n",
      "Epoch: 907\t L: 0.9070\n",
      "Epoch: 908\t L: 0.9070\n",
      "Epoch: 909\t L: 0.9070\n",
      "Epoch: 910\t L: 0.9070\n",
      "Epoch: 911\t L: 0.9070\n",
      "Epoch: 912\t L: 0.9070\n",
      "Epoch: 913\t L: 0.9070\n",
      "Epoch: 914\t L: 0.9070\n",
      "Epoch: 915\t L: 0.9070\n",
      "Epoch: 916\t L: 0.9070\n",
      "Epoch: 917\t L: 0.9070\n",
      "Epoch: 918\t L: 0.9070\n",
      "Epoch: 919\t L: 0.9070\n",
      "Epoch: 920\t L: 0.9070\n",
      "Epoch: 921\t L: 0.9070\n",
      "Epoch: 922\t L: 0.9070\n",
      "Epoch: 923\t L: 0.9070\n",
      "Epoch: 924\t L: 0.9070\n",
      "Epoch: 925\t L: 0.9070\n",
      "Epoch: 926\t L: 0.9070\n",
      "Epoch: 927\t L: 0.9070\n",
      "Epoch: 928\t L: 0.9070\n",
      "Epoch: 929\t L: 0.9070\n",
      "Epoch: 930\t L: 0.9070\n",
      "Epoch: 931\t L: 0.9070\n",
      "Epoch: 932\t L: 0.9070\n",
      "Epoch: 933\t L: 0.9070\n",
      "Epoch: 934\t L: 0.9070\n",
      "Epoch: 935\t L: 0.9070\n",
      "Epoch: 936\t L: 0.9070\n",
      "Epoch: 937\t L: 0.9070\n",
      "Epoch: 938\t L: 0.9070\n",
      "Epoch: 939\t L: 0.9070\n",
      "Epoch: 940\t L: 0.9070\n",
      "Epoch: 941\t L: 0.9070\n",
      "Epoch: 942\t L: 0.9070\n",
      "Epoch: 943\t L: 0.9070\n",
      "Epoch: 944\t L: 0.9070\n",
      "Epoch: 945\t L: 0.9070\n",
      "Epoch: 946\t L: 0.9070\n",
      "Epoch: 947\t L: 0.9070\n",
      "Epoch: 948\t L: 0.9070\n",
      "Epoch: 949\t L: 0.9070\n",
      "Epoch: 950\t L: 0.9070\n",
      "Epoch: 951\t L: 0.9070\n",
      "Epoch: 952\t L: 0.9070\n",
      "Epoch: 953\t L: 0.9070\n",
      "Epoch: 954\t L: 0.9070\n",
      "Epoch: 955\t L: 0.9070\n",
      "Epoch: 956\t L: 0.9070\n",
      "Epoch: 957\t L: 0.9070\n",
      "Epoch: 958\t L: 0.9070\n",
      "Epoch: 959\t L: 0.9070\n",
      "Epoch: 960\t L: 0.9070\n",
      "Epoch: 961\t L: 0.9070\n",
      "Epoch: 962\t L: 0.9070\n",
      "Epoch: 963\t L: 0.9070\n",
      "Epoch: 964\t L: 0.9070\n",
      "Epoch: 965\t L: 0.9070\n",
      "Epoch: 966\t L: 0.9070\n",
      "Epoch: 967\t L: 0.9070\n",
      "Epoch: 968\t L: 0.9070\n",
      "Epoch: 969\t L: 0.9070\n",
      "Epoch: 970\t L: 0.9070\n",
      "Epoch: 971\t L: 0.9070\n",
      "Epoch: 972\t L: 0.9070\n",
      "Epoch: 973\t L: 0.9070\n",
      "Epoch: 974\t L: 0.9070\n",
      "Epoch: 975\t L: 0.9070\n",
      "Epoch: 976\t L: 0.9070\n",
      "Epoch: 977\t L: 0.9070\n",
      "Epoch: 978\t L: 0.9070\n",
      "Epoch: 979\t L: 0.9070\n",
      "Epoch: 980\t L: 0.9070\n",
      "Epoch: 981\t L: 0.9070\n",
      "Epoch: 982\t L: 0.9070\n",
      "Epoch: 983\t L: 0.9070\n",
      "Epoch: 984\t L: 0.9070\n",
      "Epoch: 985\t L: 0.9070\n",
      "Epoch: 986\t L: 0.9070\n",
      "Epoch: 987\t L: 0.9070\n",
      "Epoch: 988\t L: 0.9070\n",
      "Epoch: 989\t L: 0.9070\n",
      "Epoch: 990\t L: 0.9070\n",
      "Epoch: 991\t L: 0.9070\n",
      "Epoch: 992\t L: 0.9070\n",
      "Epoch: 993\t L: 0.9070\n",
      "Epoch: 994\t L: 0.9070\n",
      "Epoch: 995\t L: 0.9070\n",
      "Epoch: 996\t L: 0.9070\n",
      "Epoch: 997\t L: 0.9070\n",
      "Epoch: 998\t L: 0.9070\n",
      "Epoch: 999\t L: 0.9070\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Binary cross entropy \n",
    "\n",
    "optimiser = torch.optim.Adam(py_model.parameters(), lr=0.0001)\n",
    "\n",
    "n_epoch = 1000\n",
    "losses = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Reset the gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    y_hat = py_model(x_train_tensor)\n",
    "    #print(y_hat.shape)\n",
    "\n",
    "    # compute loss\n",
    "    loss = criterion(y_hat, y_train_tensor)\n",
    "    loss_np = loss.detach().cpu().numpy()    #Keep Track of losses\n",
    "    losses.append(loss_np)\n",
    "\n",
    "    # Backward pass (compute the gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters (weight and bias)\n",
    "    optimiser.step()\n",
    "\n",
    "    # print(f\"Epoch: {epoch}\\t w: {model.linear.weight.data[0]}\\t b: {model.linear.bias.data[0]:.4f} \\t L: {loss:.4f}\")\n",
    "    print(f\"Epoch: {epoch}\\t L: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Graph it out\n",
    "\n",
    "plt.plot(range(n_epoch), losses)\n",
    "plt.ylabel(\"Loss/Error\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 3, 0, 2, 0, 4, 3, 3, 3, 2, 3, 0, 4, 0, 0, 3, 3, 3, 0, 1, 1, 3, 0,\n",
       "        2, 3, 4, 2, 1, 4, 0, 3, 0, 0, 2, 0, 0, 1, 3, 3, 2, 0, 1, 4, 2, 0, 2, 3,\n",
       "        3, 1, 4, 2, 3, 3, 3, 4, 0, 0, 0, 4, 3, 0, 3, 0, 3, 3, 0, 4, 0, 4, 0, 1,\n",
       "        0, 2, 2, 1, 1, 1, 3, 0, 0, 1, 1, 0, 1, 4, 0, 1, 4, 1, 1, 4, 3, 3, 0, 0,\n",
       "        3, 1, 1, 0, 0, 3, 1, 0, 4, 1, 4, 4, 4, 0, 0, 3, 1, 4, 0, 2, 0, 0, 3, 0,\n",
       "        4, 3, 1, 3, 0, 2, 4, 0, 4, 3, 1, 0, 2, 1, 2, 4, 1, 2, 3, 0, 4, 2, 4, 4,\n",
       "        4, 3, 4, 1, 4, 0, 3, 0, 1, 0, 2, 0, 4, 1, 1, 2, 0, 4, 4, 4, 3, 4, 2, 4,\n",
       "        4, 4, 3, 1, 3, 2, 2, 3, 3, 3, 3, 1, 2, 0, 1, 0, 0, 0, 1, 3, 0, 2, 2, 2,\n",
       "        0, 1, 2, 3, 3, 0, 1, 4, 0, 0, 1, 0, 1, 3, 0, 2, 1, 3, 4, 2, 0, 1, 4, 3,\n",
       "        3, 1, 1, 4, 2, 1, 3, 2, 2, 3, 2, 2, 1, 3, 2, 0, 4, 3, 4, 2, 4, 4, 1, 1,\n",
       "        1, 2, 1, 0, 0, 0, 4, 4, 3, 4, 4, 0, 4], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = py_model.forward(x_test_tensor) # prediction is in one-hot encoded form\n",
    "binary_y_predictions= torch.argmax(y_predictions, dim=1) #convert it back to a one-dimensional tensor with binary values (0 or 1).\n",
    "binary_y_predictions = binary_y_predictions.to(torch.int64)\n",
    "\n",
    "\n",
    "y_predictions\n",
    "binary_y_predictions\n",
    "#y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6759\n",
      "Precision: 0.6779\n",
      "Recall: 0.6759\n",
      "F1 Score: 0.6727\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.79      0.70        52\n",
      "         1.0       0.64      0.60      0.62        50\n",
      "         2.0       0.65      0.49      0.56        49\n",
      "         3.0       0.62      0.65      0.64        52\n",
      "         4.0       0.86      0.84      0.85        50\n",
      "\n",
      "    accuracy                           0.68       253\n",
      "   macro avg       0.68      0.67      0.67       253\n",
      "weighted avg       0.68      0.68      0.67       253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Assuming y_predictions contains the predicted class indices\n",
    "# and y_test_tensor contains the true class indices\n",
    "\n",
    "# Sample data (replace these with your actual predictions and true labels)\n",
    "\n",
    "# Convert PyTorch tensors to numpy arrays\n",
    "multi_y_predictions = y_predictions.detach().cpu().numpy()\n",
    "multi_y_true = y_test_tensor.detach().cpu().numpy()\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "predicted_labels = np.argmax(multi_y_predictions, axis=1)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(multi_y_true, predicted_labels)\n",
    "precision = precision_score(multi_y_true, predicted_labels, average='weighted')\n",
    "recall = recall_score(multi_y_true, predicted_labels, average='weighted')\n",
    "f1 = f1_score(multi_y_true, predicted_labels, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(multi_y_true, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
