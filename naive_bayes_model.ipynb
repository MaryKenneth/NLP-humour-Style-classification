{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maryk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maryk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\maryk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from split_data_utils import train_test_spliting\n",
    "from data_preprocessing import lemmatize_text_with_pos, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Humour_style.xlsx\")   # Read Excel dataset \n",
    "df = df[['JOKES', 'LABELS']]              # Extract Only the Jokes and Labels Column\n",
    "df = df[:1263]                            # Read all data from row one to row 1263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset Into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (1010,)\n",
      "y_train (1010,)\n",
      "x_test (253,)\n",
      "y_test (253,)\n",
      "['4 ways to become a better risk taker'\n",
      " '“Never argue with stupid people, they will drag you down to their level and then beat you with experience.”'\n",
      " '“Worrying is like paying a debt you don’t owe.”' ...\n",
      " \"Worker dies at minnesota vikings' stadium construction site\"\n",
      " \"sharps' injuries could pose hiv, hepatitis risk to surgeons\"\n",
      " \"My set is full of them, but I have a go to bit about how awful it is being a fat chick with small tits that almost always saves me when I'm faltering.\"]\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "seed = 100\n",
    "x_train, x_test, y_train, y_test = train_test_spliting(df,train_ratio,seed)\n",
    "\n",
    "print(\"x_train\",x_train.shape)   # Get the shape of the training features (Number of instance, number of features/column)\n",
    "print(\"y_train\",y_train.shape)   # Get the shape of the training label (Number of instance, number of column)\n",
    "print(\"x_test\",x_test.shape)\n",
    "print(\"y_test\",y_test.shape)\n",
    "\n",
    "print(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize Train and Test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize each example in the train dataset\n",
    "lemmatized_x_train  = [lemmatize_text_with_pos(example) for example in x_train]\n",
    "\n",
    "# Lemmatize each example in the test dataset\n",
    "lemmatized_x_test  = [lemmatize_text_with_pos(example) for example in x_test]\n",
    "\n",
    "x_train = np.array(lemmatized_x_train )   # Convert Train data to Numpy Array \n",
    "x_test = np.array(lemmatized_x_test)      # Convert Test data to Numpy Array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(data):\n",
    "    \"\"\"\n",
    "    Build a vocabulary from the given data.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of text examples.\n",
    "\n",
    "    Returns:\n",
    "        set: A set containing unique words in the vocabulary.\n",
    "    \"\"\"\n",
    "    all_words = set()\n",
    "    for example in data:\n",
    "        all_words.update(tokens(example))\n",
    "    return all_words\n",
    "\n",
    "# Example usage\n",
    "x_train_vocab = build_vocab(x_train)\n",
    "print(len(x_train_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_multi(x, y, smoothing=1):\n",
    "    \"\"\"\n",
    "    Implement a naive Bayes classifier for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        x (list): List of text examples.\n",
    "        y (list or array): List or array of class labels.\n",
    "        smoothing (int, optional): Laplace smoothing parameter. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing log probabilities, word counts, and word probabilities for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Given that y contains values 0, 1, 2, 3, 4 for the five classes\n",
    "    classes = np.unique(y)        # Get all unique values in y\n",
    "    vocabulary = build_vocab(x)   # Build Vocabulary from the Train data\n",
    "    N_doc = len(x)            # Get the total number of instances/row\n",
    "\n",
    "    log_probs = {}              # Dictionary to store the log prior probability of each Class\n",
    "    ex_dics = {}                # Dictionary to Store count of each word belonging to a class\n",
    "    prob_words = {}             # Store Log probability of each word belonging to a class\n",
    "\n",
    "    for class_label in classes:               #Loop through each class\n",
    "        #Get the total number of examples that belong to each class\n",
    "        N_cat = sum(y == class_label) \n",
    "\n",
    "        # Extract and concatenate examples that belong to same class. \n",
    "        # This is done for easy count of words occuring in a class       \n",
    "        examples = \" \".join(x[y == class_label]) # Extract features/text\n",
    "\n",
    "        log_prob = np.log(N_cat / N_doc)  # Get Log Prior probability (LPP) of each class\n",
    "        log_probs[class_label] = log_prob # Assign each class their LPP\n",
    "\n",
    "        ex_dic = {}\n",
    "        prob_word = {}\n",
    "\n",
    "        # Loop through Word in the Vocabulary\n",
    "        for word in vocabulary:\n",
    "            escaped_word = re.escape(word) # Escape special regex characters\n",
    "            \n",
    "            word_count = len(re.findall(escaped_word, examples)) # Using findall to get the total count of each word\n",
    "            ex_dic[word] = word_count  #Store word and their count in Dictionary \n",
    "\n",
    "            # Store Log prob of word. Round Up to 5 Decimal points\n",
    "            prob_word[word] = np.round(np.log((word_count + smoothing) / (len(tokens(examples)) + len(vocabulary))), 5)\n",
    "\n",
    "        ex_dics[class_label] = ex_dic           # Stores words and their counts for each class\n",
    "        prob_words[class_label] = prob_word     # Stores Log prob of words for each class\n",
    "\n",
    "        # Uncomment the following lines for debugging or detailed output\n",
    "        #print(f'Class {class_label}:')\n",
    "        #print(f'Examples: {tokens(examples)}')\n",
    "        #print(f'Word Counts: {ex_dic}')\n",
    "        #print(f'Word Probabilities: {prob_word}\\n')\n",
    "\n",
    "    return log_probs, ex_dics, prob_words\n",
    "\n",
    "# Example usage\n",
    "#x_train = [\"This is a positive example.\", \"Another positive example.\", \"A negative example.\", \"A neutral example.\"]\n",
    "#y_train = [0, 0, 1, 2]  # Assuming y contains values 0, 1, 2, 3 for the four classes\n",
    "\n",
    "log_probs, ex_dics, prob_words = naive_bayes_multi(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -1.5150282279630256, 1: -1.7027698522263195, 2: -1.764414015337526, 3: -1.6094379124341003, 4: -1.4839836062810654}\n"
     ]
    }
   ],
   "source": [
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction using the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes_batch(examples, log_probs, prob_words):\n",
    "    \"\"\"\n",
    "    Predicts using the Naive Bayes classifier for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        x (list): List of text examples.\n",
    "        log_probs (Dic): Log Prior Probability of each class\n",
    "        prob_words (Dic): Log Probability of each word belonging to a class\n",
    "\n",
    "    Returns:\n",
    "        List: List containing prediction of each test example.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for example in examples:\n",
    "        # Tokenize the example\n",
    "        example_tokens = tokens(example)\n",
    "\n",
    "        # Calculate the log likelihoods for each class\n",
    "        class_likelihoods = {}\n",
    "        for class_label, log_prob in log_probs.items():\n",
    "            class_likelihood = log_prob + sum(prob_words[class_label].get(word, 0) for word in example_tokens)\n",
    "            class_likelihoods[class_label] = class_likelihood\n",
    "\n",
    "        # Make a prediction based on the class with the highest likelihood\n",
    "        prediction = max(class_likelihoods, key=class_likelihoods.get)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Calling the Train and Predict Method\n",
    "log_probs, ex_dics, prob_words = naive_bayes_multi(x_train, y_train)\n",
    "predicted_labels = predict_naive_bayes_batch(x_test, log_probs, prob_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.75%\n",
      "Class 0: Precision=0.6508, Recall=0.7885, F1-score=0.7130\n",
      "Class 1: Precision=0.7273, Recall=0.6667, F1-score=0.6957\n",
      "Class 2: Precision=0.5750, Recall=0.5476, F1-score=0.5610\n",
      "Class 3: Precision=0.6667, Recall=0.7667, F1-score=0.7132\n",
      "Class 4: Precision=1.0000, Recall=0.7255, F1-score=0.8409\n",
      "\n",
      "Macro-Averaged Metrics:\n",
      "Precision: 72.39%, Recall: 69.90%, F1-Score: 70.48%\n",
      "\n",
      "Micro-Averaged Metrics:\n",
      "Precision: 70.75%, Recall: 70.75%, F1-Score: 70.75%\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(predicted_labels, actual_labels):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        predicted_labels (list): List of predicted class labels.\n",
    "        actual_labels (list): List of actual class labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy, precision, recall, F1-score, true_positives, false_positives, and false_negatives.\n",
    "    \"\"\"\n",
    "    correct_predictions = sum(1 for pred, actual in zip(predicted_labels, actual_labels) if pred == actual)\n",
    "    accuracy = correct_predictions / len(actual_labels)\n",
    "\n",
    "    precision = {\n",
    "        class_label: sum(1 for pred, actual in zip(predicted_labels, actual_labels) if pred == class_label and actual == class_label) / predicted_labels.count(class_label)\n",
    "        for class_label in set(actual_labels)\n",
    "    }\n",
    "\n",
    "    recall = {\n",
    "        class_label: sum(1 for pred, actual in zip(predicted_labels, actual_labels) if pred == class_label and actual == class_label) / sum(1 for actual in actual_labels if actual == class_label)\n",
    "        for class_label in set(actual_labels)\n",
    "    }\n",
    "\n",
    "    f1_score = {\n",
    "        class_label: 2 * (precision[class_label] * recall[class_label]) / (precision[class_label] + recall[class_label])\n",
    "        for class_label in set(actual_labels)\n",
    "    }\n",
    "\n",
    "    true_positives = {\n",
    "        class_label: sum(1 for pred, actual in zip(predicted_labels, actual_labels) if pred == class_label and actual == class_label)\n",
    "        for class_label in set(actual_labels)\n",
    "    }\n",
    "\n",
    "    false_positives = {\n",
    "        class_label: sum(1 for pred, actual in zip(predicted_labels, actual_labels) if pred == class_label and actual != class_label)\n",
    "        for class_label in set(actual_labels)\n",
    "    }\n",
    "\n",
    "    false_negatives = {\n",
    "        class_label: sum(1 for pred, actual in zip(predicted_labels, actual_labels) if pred != class_label and actual == class_label)\n",
    "        for class_label in set(actual_labels)\n",
    "    }\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Usage:\n",
    "metrics = calculate_metrics(predicted_labels, y_test)\n",
    "print(f'Accuracy: {metrics[\"accuracy\"]:.2%}')\n",
    "\n",
    "# Print precision, recall, and F1-score for each class\n",
    "for class_label in set(y_test):\n",
    "    print(f'Class {class_label}: Precision={metrics[\"precision\"][class_label]:.4f}, Recall={metrics[\"recall\"][class_label]:.4f}, F1-score={metrics[\"f1_score\"][class_label]:.4f}')\n",
    "\n",
    "# Calculate macro-averaged precision, recall, and F1-score\n",
    "macro_precision = sum(metrics['precision'].values()) / len(set(y_test))\n",
    "macro_recall = sum(metrics['recall'].values()) / len(set(y_test))\n",
    "macro_f1_score = sum(metrics['f1_score'].values()) / len(set(y_test))\n",
    "\n",
    "# Print macro-averaged metrics\n",
    "print(f'\\nMacro-Averaged Metrics:')\n",
    "print(f'Precision: {macro_precision:.2%}, Recall: {macro_recall:.2%}, F1-Score: {macro_f1_score:.2%}')\n",
    "\n",
    "# Calculate micro-averaged precision, recall, and F1-score\n",
    "micro_true_positives = sum(metrics['true_positives'].values())\n",
    "micro_false_positives = sum(metrics['false_positives'].values())\n",
    "micro_false_negatives = sum(metrics['false_negatives'].values())\n",
    "\n",
    "micro_precision = micro_true_positives / (micro_true_positives + micro_false_positives) if (micro_true_positives + micro_false_positives) > 0 else 0\n",
    "micro_recall = micro_true_positives / (micro_true_positives + micro_false_negatives) if (micro_true_positives + micro_false_negatives) > 0 else 0\n",
    "micro_f1_score = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "# Print micro-averaged metrics\n",
    "print(f'\\nMicro-Averaged Metrics:')\n",
    "print(f'Precision: {micro_precision:.2%}, Recall: {micro_recall:.2%}, F1-Score: {micro_f1_score:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
